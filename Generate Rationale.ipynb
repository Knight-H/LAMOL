{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8, 9]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6,7,8,9,]\n",
    "b = [7,8,9]\n",
    "a[-len(b):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = set(a)\n",
    "len(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where([0,0,0,1])[0][0]\n",
    "np.where([False,False,False,True])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.array(a+[3]) == 3)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([6, 15, 18, 20, 138, 167, 185, 209, 242, 248, 272, 40, 70, 103, 108, 156, 198, 124, 88, 97, 142, 213, 111, 22, 10, 160, 163, 224, 59, 262, 158, 282, 17, 34, 23, 130, 151, 286, 241, 82, 62, 179, 246, 254, 244, 240, 104, 135, 132, 145, 141, 76, 184, 281, 74, 166, 273, 297, 131, 79, 249, 48, 9, 137, 192, 144, 212, 278, 200, 304, 271, 143, 171, 3, 22, 262, 137, 17, 184, 18, 244, 23, 59, 209, 171, 79, 141, 163, 198, 179, 10, 97, 297, 224, 97, 248, 70, 79, 244, 167, 137, 10, 272, 145, 18, 70, 142, 151, 17, 76, 272, 138, 273, 192, 15, 184, 241, 3, 248, 160, 15, 103, 213, 124, 304, 34, 145, 143, 281, 108, 22, 248, 271, 242, 198, 88, 184, 18, 286, 62, 23, 212, 179, 224, 282, 82, 209, 70, 151, 10, 17, 271, 213, 18, 160, 103, 278, 79, 132, 244, 3, 143, 74, 212, 70, 40, 15, 273, 213, 145, 209, 185, 249, 9, 192, 144, 272, 143, 281, 145, 213, 70, 59, 74, 273, 97, 40, 241, 141, 142, 62, 151, 124, 304, 74, 200, 20, 124, 142, 249, 192, 111, 179, 76, 262, 97, 241, 151, 254, 212, 108, 156, 262, 82, 185, 248, 240, 145, 103, 141, 249, 9, 48, 40, 142, 34, 304, 254, 297, 273, 131, 124, 17, 151, 242, 62, 144, 184])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([6, 15, 18, 20, 138, 167, 185, 209, 242, 248, 272, 40, 70, 103, 108, 156, 198, 124, 88, 97, 142, 213, 111, 22, 10, 160, 163, 224, 59, 262, 158, 282, 17, 34, 23, 130, 151, 286, 241, 82, 62, 179, 246, 254, 244, 240, 104, 135, 132, 145, 141, 76, 184, 281, 74, 166, 273, 297, 131, 79, 249, 48, 9, 137, 192, 144, 212, 278, 200, 304, 271, 143, 171, 3, 22, 262, 137, 17, 184, 18, 244, 23, 59, 209, 171, 79, 141, 163, 198, 179, 10, 97, 297, 224, 97, 248, 70, 79, 244, 167, 137, 10, 272, 145, 18, 70, 142, 151, 17, 76, 272, 138, 273, 192, 15, 184, 241, 3, 248, 160, 15, 103, 213, 124, 304, 34, 145, 143, 281, 108, 22, 248, 271, 242, 198, 88, 184, 18, 286, 62, 23, 212, 179, 224, 282, 82, 209, 70, 151, 10, 17, 271, 213, 18, 160, 103, 278, 79, 132, 244, 3, 143, 74, 212, 70, 40, 15, 273, 213, 145, 209, 185, 249, 9, 192, 144, 272, 143, 281, 145, 213, 70, 59, 74, 273, 97, 40, 241, 141, 142, 62, 151, 124, 304, 74, 200, 20, 124, 142, 249, 192, 111, 179, 76, 262, 97, 241, 151, 254, 212, 108, 156, 262, 82, 185, 248, 240, 145, 103, 141, 249, 9, 48, 40, 142, 34, 304, 254, 297, 273, 131, 124, 17, 151, 242, 62, 144, 184, 213, 62, 179, 242, 244, 74, 143, 224, 158, 156, 282, 17, 76, 254, 281, 135, 22, 104, 10, 130, 171, 20, 151, 48, 144, 70, 249, 131, 158, 212, 15, 297, 143, 141, 135, 151, 20, 9, 15, 156, 111, 79, 18, 242, 143, 62, 179, 132, 185, 141, 166, 224, 138, 145, 103, 163, 158, 262, 70, 137, 244, 184, 212, 15, 246, 282, 104, 9, 143, 142, 249, 160, 240, 103, 273, 278, 88, 10, 48, 192, 163, 3, 167, 209, 20, 104, 212, 137, 130, 74, 297, 179, 271, 166, 131, 15, 132, 18, 240, 167, 6, 34, 10, 254, 124, 244, 304, 212, 156, 48, 145, 135, 271, 76, 163, 209, 184, 297, 15, 262, 166, 111, 138, 254, 143, 108, 131, 145, 23, 179, 192, 17, 171, 160, 34, 246, 281, 137, 241, 97, 278, 124])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a+[3]).index(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d61334d63904>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m TASK_DICT = {\n\u001b[1;32m      7\u001b[0m     \"movie\": {\n\u001b[0;32m----> 8\u001b[0;31m                \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"movie_train.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"movie_dev.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"movie_test.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def get_gen_token(task):\n",
    "    if True: #args.add_task_tokens\n",
    "        return '__' + task + '__'\n",
    "    else:\n",
    "        return '__gen__'\n",
    "TASK_DICT = {\n",
    "    \"movie\": {\n",
    "               \"train\":os.path.join(args.data_dir,\"movie_train.json\"),\n",
    "               \"eval\":os.path.join(args.data_dir,\"movie_dev.json\"),\n",
    "               \"test\":os.path.join(args.data_dir,\"movie_test.json\"),\n",
    "               \"n_train_epochs\": args.n_train_epochs \n",
    "    },\n",
    "    \"boolq\": {\n",
    "               \"train\":os.path.join(args.data_dir,\"boolq_train.json\"),\n",
    "               \"eval\":os.path.join(args.data_dir,\"boolq_dev.json\"),\n",
    "               \"test\":os.path.join(args.data_dir,\"boolq_test.json\"),\n",
    "               \"n_train_epochs\": args.n_train_epochs \n",
    "    },\n",
    "    \"scifact\": {\n",
    "               \"train\":os.path.join(args.data_dir,\"scifact_train.json\"),\n",
    "               \"eval\":os.path.join(args.data_dir,\"scifact_dev.json\"),\n",
    "               \"test\":os.path.join(args.data_dir,\"scifact_test.json\"),\n",
    "               \"n_train_epochs\": args.n_train_epochs \n",
    "    }\n",
    "}\n",
    "special_tokens = {\"ans_token\":'__ans__', \"pad_token\":'__pad__', \"unk_token\":'__unk__', \"eos_token\": '<|endoftext|>'}\n",
    "    if args.use_sep:\n",
    "        special_tokens[\"sep_token\"] = '__sep__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_load = 'boolq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__boolq__'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_token = get_gen_token(task_load)\n",
    "gen_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task_id is 0,1,2 (for looped)\n",
    "# model is the loaded state dict\n",
    "# first_task is True/False\n",
    "# to_freeze is the layers to freeze\n",
    "model = train([task_id], model,first_task,to_freeze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boolq']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_tasks = [\"boolq\", \"movie\", \"scifact\"]\n",
    "task_ids = [0]\n",
    "tasks = [arg_tasks[task_id] for task_id in task_ids]\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data directory of train in the task\n",
    "train_dataset = [TASK_DICT[t][\"train\"] for t in tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev_task is the previous task (what if there first??)\n",
    "# create extra data with the (current task, prev task, model, blank list?)\n",
    "train_extra_data = []\n",
    "if \"lll\" in args.seq_train_type and not args.skip_tasks:\n",
    "        prev_task = args.tasks[task_ids[0]-1]\n",
    "        with torch.no_grad():\n",
    "            create_extra_data(tasks[0], prev_task, model, train_extra_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Create Extra Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extra_data(task, prev_task, model, train_extra_data):\n",
    "    # If Getting Real Samples\n",
    "    if args.real_sample:\n",
    "        logger.info(f\"using real data as extra data\")\n",
    "        return get_real_data(task, train_extra_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    task_cnt = args.tasks.index(task)              # This would be 0 since tasks will have 1 length?\n",
    "    model_dir = get_model_dir([prev_task])         # Path concat (args.model_dir_root, tasks[0])\n",
    "    gen_path = os.path.join(model_dir,\"lm.csv\")    \n",
    "    if os.path.exists(gen_path):                   # Find if gen_path exists\n",
    "        logger.info(f\"extra data exists in {gen_path}, read it!\")\n",
    "        return read_extra_data(gen_path, train_extra_data) \n",
    "    gen_size = DATA_ATTRS[task][\"train\"][\"data_size\"]     # a json.load from os.path.join(BASE_DIR,\"data_attrs.json\"), should be data size\n",
    "    gen_size = int(np.ceil(gen_size * args.gen_lm_sample_percentage)) # multiply datasize with sample percentage and get ceil\n",
    "    gen_size -= (gen_size % task_cnt)   #maybe task_cnt not 0?!?! but how?  is this just add? () is negative right?\n",
    "\n",
    "    if args.debug:\n",
    "        gen_size = task_cnt\n",
    "\n",
    "    model.eval()\n",
    "    logger.info(f\"generating extra data!\")\n",
    "    need_process = OrderedDict()\n",
    "    qa_results = []\n",
    "    # For a single task_name 'boolq', make qa_results a torch tensor with a single value of the task_name token \n",
    "    for task_name in args.tasks[:task_cnt]:\n",
    "        qa_results.extend([torch.tensor([SPECIAL_TOKEN_IDS[task_name]]) for _ in range(gen_size//task_cnt)])\n",
    "        \n",
    "    # Initialize empty tensors for gensize , n_layers ?\n",
    "    all_pasts = [[\n",
    "        torch.empty(2, MODEL_CONFIG.n_head, 0, MODEL_CONFIG.n_embd//MODEL_CONFIG.n_head,\n",
    "            dtype=torch.float if args.fp32 else torch.half).cuda()\n",
    "        for _ in range(gen_size)\n",
    "    ] for __ in range(MODEL_CONFIG.n_layer)]\n",
    "    # a list of max_len in the range of gen_size?\n",
    "    max_tot_lens = [args.max_len for _ in range(gen_size)]\n",
    "\n",
    "    # For i in range of gen_size, \n",
    "    for i in range(gen_size):\n",
    "        need_process.update([[i, None]])  # Need_process is an OrderedDict Up Above\n",
    "        if len(need_process) > int(args.memory_sizes[0] * 0.12): #?!?!? check if len(need_process) > memorysize*12%?\n",
    "            sample_sequence(model, need_process, qa_results, all_pasts, max_tot_lens)\n",
    "    sample_sequence(model, need_process, qa_results, all_pasts, max_tot_lens)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    qa_results = [res.tolist() for res in qa_results]\n",
    "    train_extra_data.extend(qa_results)\n",
    "    qa_results = [TOKENIZER.decode(res) for res in qa_results]\n",
    "    \n",
    "    # Write qa_results as csv, all in gen_path\n",
    "    write_extra_data(gen_path, qa_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([123]),\n",
       " tensor([123]),\n",
       " tensor([123]),\n",
       " tensor([123]),\n",
       " tensor([123]),\n",
       " tensor([123]),\n",
       " tensor([123]),\n",
       " tensor([123]),\n",
       " tensor([123]),\n",
       " tensor([123])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_results = []\n",
    "gen_size = 10\n",
    "task_cnt=1\n",
    "qa_results.extend([torch.tensor([123]) for _ in range(gen_size//task_cnt)])\n",
    "qa_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_root = \"_test_model_knight\"\n",
    "def get_model_dir(tasks):\n",
    "    return os.path.join(model_dir_root, tasks[0]) \n",
    "MODEL_CONFIG = {\n",
    "  \"activation_function\": \"gelu_new\",\n",
    "  \"architectures\": [\n",
    "    \"GPT2LMHeadModel\"\n",
    "  ],\n",
    "  \"attn_pdrop\": 0.1,\n",
    "  \"bos_token_id\": 50256,\n",
    "  \"embd_pdrop\": 0.1,\n",
    "  \"eos_token_id\": 50256,\n",
    "  \"finetuning_task\": None,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_epsilon\": 1e-05,\n",
    "  \"model_type\": \"gpt2\",\n",
    "  \"n_ctx\": 1024,\n",
    "  \"n_embd\": 768,\n",
    "  \"n_head\": 12,\n",
    "  \"n_layer\": 12,\n",
    "  \"n_positions\": 1024,\n",
    "  \"num_labels\": 1,\n",
    "  \"output_attentions\": False,\n",
    "  \"output_hidden_states\": False,\n",
    "  \"pruned_heads\": {},\n",
    "  \"resid_pdrop\": 0.1,\n",
    "  \"summary_activation\": None,\n",
    "  \"summary_first_dropout\": 0.1,\n",
    "  \"summary_proj_to_labels\": True,\n",
    "  \"summary_type\": \"cls_index\",\n",
    "  \"summary_use_proj\": True,\n",
    "  \"task_specific_params\": {\n",
    "    \"text-generation\": {\n",
    "      \"do_sample\": True,\n",
    "      \"max_length\": 50\n",
    "    }\n",
    "  },\n",
    "  \"torchscript\": False,\n",
    "  \"vocab_size\": 50261\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating extra data!\n",
      "qa_results length : 1273\n",
      "[tensor([123]), tensor([123]), tensor([123]), tensor([123]), tensor([123])]\n",
      "all_pasts len: 12\n",
      "[tensor([], device='cuda:0', size=(2, 12, 0, 64), dtype=torch.float16), tensor([], device='cuda:0', size=(2, 12, 0, 64), dtype=torch.float16)]\n",
      "max_tot_lens len : 1273\n",
      "[1024, 1024, 1024]\n",
      "OrderedDict([(0, None), (1, None), (2, None), (3, None), (4, None), (5, None), (6, None), (7, None), (8, None), (9, None), (10, None), (11, None), (12, None)])\n",
      "\n",
      "THIS IS INPUT ID tensor([[123],\n",
      "        [123],\n",
      "        [123],\n",
      "        [123],\n",
      "        [123],\n",
      "        [123],\n",
      "        [123],\n",
      "        [123],\n",
      "        [123],\n",
      "        [123],\n",
      "        [123],\n",
      "        [123],\n",
      "        [123]])\n",
      "THIS IS cur_id 12\n",
      "THIS IS batch_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "THIS IS past len 12\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-7c958130f7a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneed_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0msample_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_process\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_pasts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tot_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-126-34b69badcf9f>\u001b[0m in \u001b[0;36msample_sequence\u001b[0;34m(model, need_process, qa_results, all_pasts, max_tot_lens)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mlayer_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_CONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_layer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mpast\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mall_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mall_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "gen_lm_sample_percentage = 0.2\n",
    "\n",
    "arg_tasks = [\"boolq\", \"movie\", \"scifact\"]\n",
    "arg_max_len = MODEL_CONFIG[\"n_positions\"]\n",
    "# gpus = GPUtil.getGPUs()\n",
    "# gpu_names = [gpus[device_id].name for device_id in args.device_ids]\n",
    "# if not all(any(turing_arch in gpu_name for turing_arch in TURING_ARCHS) for gpu_name in gpu_names):\n",
    "#     logger.warning('Not all gpus support fp16 training! Will use fp32 instead.')\n",
    "#     args.fp32 = True\n",
    "# if args.model_name == \"openai-gpt\":\n",
    "#     args.fp32 = True  # openai-gpt currently doesn't support fp16\n",
    "# if not args.fp32:\n",
    "#     global MEMORY_FACTOR\n",
    "#     MEMORY_FACTOR = dict([k, v*1.4] for k, v in MEMORY_FACTOR.items())\n",
    "# args.memory_sizes = [gpus[device_id].memoryTotal for device_id in args.device_ids]\n",
    "# args.memory_sizes[0] = args.memory_sizes[0] * (1 - 0.04 * (args.n_gpus-1))\n",
    "# for i in range(1, args.n_gpus):\n",
    "#     args.memory_sizes[i] = args.memory_sizes[i] * 1.04\n",
    "# if args.train_batch_size <= 0:\n",
    "#     args.train_batch_size = [int(memory_size * MEMORY_FACTOR[args.seq_train_type]) for memory_size in args.memory_sizes]\n",
    "# if args.test_batch_size <= 0:\n",
    "#     args.test_batch_size = [int(memory_size * MEMORY_FACTOR[args.seq_train_type]) for memory_size in args.memory_sizes]\n",
    "arg_memory_sizes = [100]\n",
    "model = None\n",
    "LEN_FACTOR = 1.163\n",
    "arg_model_name = \"gpt2\"\n",
    "\n",
    "task_ids = [0]\n",
    "tasks = [arg_tasks[task_id] for task_id in task_ids]\n",
    "prev_task = tasks[task_ids[0]-1]\n",
    "\n",
    "task = tasks[0]\n",
    "task_cnt = arg_tasks.index(task)\n",
    "task_cnt = 1 # TEMP 1?!?\n",
    "\n",
    "model_dir = get_model_dir([prev_task]) \n",
    "gen_path = os.path.join(model_dir,\"lm.csv\")\n",
    "\n",
    "gen_size = 6363  # BOOLQ TRAIN DATA SIZE  : DATA_ATTRS[task][\"train\"][\"data_size\"]\n",
    "gen_size = int(np.ceil(gen_size * gen_lm_sample_percentage))\n",
    "gen_size -= (gen_size % task_cnt) \n",
    "\n",
    "print(f\"generating extra data!\")\n",
    "need_process = OrderedDict()\n",
    "qa_results = []\n",
    "for task_name in arg_tasks[:task_cnt]:\n",
    "    qa_results.extend([torch.tensor([123]) for _ in range(gen_size//task_cnt)])\n",
    "all_pasts = [[\n",
    "    torch.empty(2, MODEL_CONFIG[\"n_head\"], 0, MODEL_CONFIG[\"n_embd\"]//MODEL_CONFIG[\"n_head\"],\n",
    "        dtype=torch.half).cuda()\n",
    "    for _ in range(gen_size)\n",
    "] for __ in range(MODEL_CONFIG[\"n_layer\"])]\n",
    "max_tot_lens = [arg_max_len for _ in range(gen_size)]\n",
    "\n",
    "print(f\"qa_results length : {len(qa_results)}\")\n",
    "print(qa_results[0:5])\n",
    "print(f\"all_pasts len: {len(all_pasts)}\")\n",
    "print(all_pasts[0][:2])\n",
    "print(f\"max_tot_lens len : {len(max_tot_lens)}\")\n",
    "print(max_tot_lens[:3])\n",
    "\n",
    "for i in range(gen_size):\n",
    "    need_process.update([[i, None]])  # Need_process is an OrderedDict Up Above\n",
    "    if len(need_process) > int(arg_memory_sizes[0] * 0.12): #?!?!? check if len(need_process) > memorysize*12%?\n",
    "        print(need_process)\n",
    "        print()\n",
    "        sample_sequence(model, need_process, qa_results, all_pasts, max_tot_lens, task)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(model, need_process, qa_results, all_pasts, max_tot_lens, task):\n",
    "    while len(need_process) > 0:\n",
    "        first_id = next(iter(need_process))             # iterate over need_process, strts with 0\n",
    "        shortest_len = len(qa_results[first_id])        # qa_results a torch tensor with a single value of the task_name token with length gen\n",
    "        decode_batch_size = int(arg_memory_sizes[0] * 1.04 // (shortest_len+1)**LEN_FACTOR)\n",
    "        it = iter(need_process)\n",
    "        stop = False\n",
    "        remove_ids = []\n",
    "        while not stop:\n",
    "            batch_ids, input_ids, past = [], [], [[] for _ in range(MODEL_CONFIG[\"n_layer\"])]\n",
    "            while True:\n",
    "                try:\n",
    "                    cur_id = next(it)\n",
    "                    if len(qa_results[cur_id]) > shortest_len:\n",
    "                        stop = True\n",
    "                        break\n",
    "                    batch_ids.append(cur_id)\n",
    "                    if arg_model_name == \"gpt2\":\n",
    "                        input_ids.append(qa_results[cur_id][-1:])  # if gpt2, only put in the last token of the batch_id?\n",
    "                        for layer_id in range(MODEL_CONFIG[\"n_layer\"]):\n",
    "                            past[layer_id].append(all_pasts[layer_id][cur_id])\n",
    "                    else:\n",
    "                        input_ids.append(qa_results[cur_id])\n",
    "                    if len(input_ids) == decode_batch_size:\n",
    "                        break\n",
    "                except StopIteration:\n",
    "                    stop = True\n",
    "                    break\n",
    "            \n",
    "\n",
    "            n_inputs = len(input_ids)\n",
    "            if n_inputs == 0:\n",
    "                break\n",
    "            input_ids = torch.stack(input_ids)\n",
    "            \n",
    "            print(f\"THIS IS INPUT ID {input_ids}\")\n",
    "            print(f\"THIS IS batch_ids {batch_ids}\")\n",
    "            print(f\"THIS IS past len {len(past)}\")\n",
    "            \n",
    "            if arg_model_name == \"gpt2\":\n",
    "                for layer_id in range(MODEL_CONFIG[\"n_layer\"]):\n",
    "                    past[layer_id] = torch.stack(past[layer_id], dim=1)\n",
    "                all_outputs = model(input_ids=input_ids.cuda(), past=past) #somehow gpt2 needs to have past as input too\n",
    "            else:\n",
    "                all_outputs = model(input_ids=input_ids.cuda())\n",
    "\n",
    "            outputs = all_outputs[0]\n",
    "            if arg_model_name == \"gpt2\":\n",
    "                pasts = all_outputs[1] # save all_outputs[1] into pasts\n",
    "\n",
    "            next_logits = outputs[..., -1, :] / args.temperature_qa\n",
    "            next_tokens = logits_to_tokens(next_logits).cpu()  # top_k, top_p filtering -> softmax -> multinomial sampling\n",
    "\n",
    "            for i, cur_id in enumerate(batch_ids):\n",
    "                if next_tokens[i] == SPECIAL_TOKEN_IDS[\"eos_token\"]:\n",
    "                    ### KNIGHT ADD POSTPROCESSING ###\n",
    "                    if qa_results[cur_id][-1] in TOKENIZER.convert_tokens_to_ids(ALLOWED_TOKENS[task]):\n",
    "                        pass\n",
    "                    else:\n",
    "                        qa_results[cur_id][-2] = SPECIAL_TOKEN_IDS[\"ans_token\"]\n",
    "                        qa_results[cur_id][-1] = TOKENIZER.convert_tokens_to_ids(np.random.choice(ALLOWED_TOKENS[task]))\n",
    "                    ### END KNIGHT ADD POSTPROCESSING ###\n",
    "                    remove_ids.append(cur_id)\n",
    "                else:\n",
    "                    ### KNIGHT ADD POSTPROCESSING ###\n",
    "                    if len(qa_results[cur_id])+1 in [max_tot_lens[cur_id], args.max_len]: # CHECK!! is next_tokens really 1 token?!?\n",
    "                        if next_tokens[i] in TOKENIZER.convert_tokens_to_ids(ALLOWED_TOKENS[task]):\n",
    "                            pass\n",
    "                        else:\n",
    "                            qa_results[cur_id][-1] = SPECIAL_TOKEN_IDS[\"ans_token\"]\n",
    "                            # or use next_logits\n",
    "                            \n",
    "                            next_tokens[i] = TOKENIZER.convert_tokens_to_ids(np.random.choice(ALLOWED_TOKENS[task]))\n",
    "                            next_tokens[i] = np.max(next_logits.cpu()[TOKENIZER.convert_tokens_to_ids(ALLOWED_TOKENS[task])])\n",
    "                    ### END KNIGHT ADD POSTPROCESSING ###\n",
    "                    qa_results[cur_id] = torch.cat((qa_results[cur_id], next_tokens[i])) #append the next_token to the current qa_results\n",
    "                    if len(qa_results[cur_id]) in [max_tot_lens[cur_id], args.max_len]: # if the length is equal to the max length\n",
    "                        remove_ids.append(cur_id)                                        # append the batch_id in the remove_ids list\n",
    "                    elif arg_model_name == \"gpt2\":\n",
    "                        for layer_id in range(MODEL_CONFIG.n_layer):\n",
    "                            all_pasts[layer_id][cur_id] = pasts[layer_id][:, i].type(torch.float if args.fp32 else torch.half)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for idx in remove_ids:\n",
    "            remove_id(idx, need_process, all_pasts) # del need_process[idx], all_pasts[layer_id][idx] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEG'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(ALLOWED_TOKENS[\"movies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boolq'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_size = 201\n",
    "task_cnt = 1\n",
    "gen_size -= (gen_size % task_cnt)\n",
    "gen_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_TOKENS = {\n",
    "    \"movies\" : [\"NEG\", \"POS\"],\n",
    "    \"boolq\": [\"True\", \"False\"],\n",
    "    \"scifact\": [\"REFUTES\", \"SUPPORTS\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Sample_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(0, None),\n",
       "             (1, None),\n",
       "             (2, None),\n",
       "             (3, None),\n",
       "             (4, None),\n",
       "             (5, None),\n",
       "             (6, None),\n",
       "             (7, None),\n",
       "             (8, None),\n",
       "             (9, None)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "need_process = OrderedDict()\n",
    "for i in range(10):\n",
    "    need_process.update([[i, None]])\n",
    "need_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(need_process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<odict_iterator at 0x7f21040e6290>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(need_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(model, need_process, qa_results, all_pasts, max_tot_lens):\n",
    "    while len(need_process) > 0:\n",
    "        first_id = next(iter(need_process))             # iterate over need_process, strts with 0\n",
    "        shortest_len = len(qa_results[first_id])        # qa_results a torch tensor with a single value of the task_name token with length gen\n",
    "        decode_batch_size = int(args.memory_sizes[0] * MEMORY_FACTOR[args.seq_train_type] // (shortest_len+1)**LEN_FACTOR)\n",
    "        it = iter(need_process)\n",
    "        stop = False\n",
    "        remove_ids = []\n",
    "        while not stop:\n",
    "            batch_ids, input_ids, past = [], [], [[] for _ in range(MODEL_CONFIG.n_layer)]\n",
    "            while True:\n",
    "                try:\n",
    "                    cur_id = next(it)\n",
    "                    if len(qa_results[cur_id]) > shortest_len:\n",
    "                        stop = True\n",
    "                        break\n",
    "                    batch_ids.append(cur_id)\n",
    "                    if args.model_name == \"gpt2\":\n",
    "                        input_ids.append(qa_results[cur_id][-1:])\n",
    "                        for layer_id in range(MODEL_CONFIG.n_layer):\n",
    "                            past[layer_id].append(all_pasts[layer_id][cur_id])\n",
    "                    else:\n",
    "                        input_ids.append(qa_results[cur_id])\n",
    "                    if len(input_ids) == decode_batch_size:\n",
    "                        break\n",
    "                except StopIteration:\n",
    "                    stop = True\n",
    "                    break\n",
    "\n",
    "            n_inputs = len(input_ids)\n",
    "            if n_inputs == 0:\n",
    "                break\n",
    "            input_ids = torch.stack(input_ids)\n",
    "            if args.model_name == \"gpt2\":\n",
    "                for layer_id in range(MODEL_CONFIG.n_layer):\n",
    "                    past[layer_id] = torch.stack(past[layer_id], dim=1)\n",
    "                all_outputs = model(input_ids=input_ids.cuda(), past=past)\n",
    "            else:\n",
    "                all_outputs = model(input_ids=input_ids.cuda())\n",
    "\n",
    "            outputs = all_outputs[0]\n",
    "            if args.model_name == \"gpt2\":\n",
    "                pasts = all_outputs[1]\n",
    "\n",
    "            next_logits = outputs[..., -1, :] / args.temperature_qa\n",
    "            next_tokens = logits_to_tokens(next_logits).cpu()\n",
    "\n",
    "            for i, cur_id in enumerate(batch_ids):\n",
    "                if next_tokens[i] == SPECIAL_TOKEN_IDS[\"eos_token\"]:\n",
    "                    remove_ids.append(cur_id)\n",
    "                else:\n",
    "                    qa_results[cur_id] = torch.cat((qa_results[cur_id], next_tokens[i]))\n",
    "                    if len(qa_results[cur_id]) in [max_tot_lens[cur_id], args.max_len]:\n",
    "                        remove_ids.append(cur_id)\n",
    "                    elif args.model_name == \"gpt2\":\n",
    "                        for layer_id in range(MODEL_CONFIG.n_layer):\n",
    "                            all_pasts[layer_id][cur_id] = pasts[layer_id][:, i].type(torch.float if args.fp32 else torch.half)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for idx in remove_ids:\n",
    "            remove_id(idx, need_process, all_pasts) # del need_process[idx], all_pasts[layer_id][idx] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(need_process)\n",
    "batch_ids, input_ids, past = [], [], [[] for _ in range(12)]\n",
    "while True:\n",
    "    try:\n",
    "        cur_id = next(it)                           #cursor_id of need_process??\n",
    "        if len(qa_results[cur_id]) > shortest_len:  # Check, Stop if qa_results[cur_id] > shortest_len\n",
    "            stop = True\n",
    "            break\n",
    "        batch_ids.append(cur_id)\n",
    "        if args.model_name == \"gpt2\":\n",
    "            input_ids.append(qa_results[cur_id][-1:])         # append input_ids with qa_results[cur_id][-1:]\n",
    "            for layer_id in range(MODEL_CONFIG.n_layer):       # for all layers\n",
    "                past[layer_id].append(all_pasts[layer_id][cur_id])     # append past[layer_id] with all_past[Layer_id][cur_id]\n",
    "        else:\n",
    "            input_ids.append(qa_results[cur_id])\n",
    "        if len(input_ids) == decode_batch_size:                # break if len(input_ids) == decode_batch_size\n",
    "            break\n",
    "    except StopIteration:\n",
    "        stop = True\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(need_process))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
